import os
import pandas as pd
import torch
import torch.nn.functional as F
from polygon import RESTClient
from transformers import BertTokenizer, BertForSequenceClassification
from tqdm import tqdm
from sklearn.preprocessing import MinMaxScaler
import numpy as np


class PolygonDataPipeline:
    def __init__(self, cfg):
        """Hydra config ê¸°ë°˜ ì´ˆê¸°í™”"""
        self.cfg = cfg
        self.ticker = cfg.data.ticker
        self.start_date = cfg.data.start_date
        self.end_date = cfg.data.end_date
        self.news_limit = cfg.data.news_limit
        self.window_size = cfg.data.window_size
        self.api_key = cfg.api_key
        
        self.client = RESTClient(self.api_key)
        
        # FinBERT ì„¤ì •
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"Using device: {self.device}")
        
        self.tokenizer = BertTokenizer.from_pretrained('ProsusAI/finbert')
        self.model = BertForSequenceClassification.from_pretrained('ProsusAI/finbert').to(self.device)
        self.model.eval()

    def fetch_prices(self):
        """ì£¼ê°€ ë°ì´í„° ìˆ˜ì§‘ (OHLCV + Change)"""
        print(f"\n[1/3] Fetching Price Data for {self.ticker}...")
        aggs = []
        try:
            req_start = (pd.to_datetime(self.start_date) - pd.Timedelta(days=5)).strftime("%Y-%m-%d")
            
            for a in self.client.list_aggs(
                ticker=self.ticker,
                multiplier=1,
                timespan="day",
                from_=req_start,
                to=self.end_date,
                limit=50000
            ):
                aggs.append({
                    'Date': pd.to_datetime(a.timestamp, unit='ms').date(),
                    'Open': a.open,
                    'High': a.high,
                    'Low': a.low,
                    'Close': a.close,
                    'Volume': a.volume
                })
        except Exception as e:
            print(f"Error fetching prices: {e}")
        
        df = pd.DataFrame(aggs)
        if not df.empty:
            df = df.set_index('Date').sort_index()
            df = df.loc[pd.to_datetime(self.start_date).date() : pd.to_datetime(self.end_date).date()]
        
        return df

    def fetch_news_with_volume(self):
        """
        ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ (ê°œìˆ˜ ì •ë³´ + ìƒ˜í”Œë§)
        - News_Count: ê° ë‚ ì§œì˜ ì‹¤ì œ ë‰´ìŠ¤ ê°œìˆ˜ (ëª¨ë‘ ì…ˆ)
        - Sentiment: ë‚ ì§œë‹¹ ìµœëŒ€ max_per_dayê°œ ìƒ˜í”Œë§í•˜ì—¬ ë¶„ì„
        """
        print(f"\n[2/3] Fetching News Data for {self.ticker}...")
        
        max_per_day = 10  # ë‚ ì§œë‹¹ ìµœëŒ€ ë¶„ì„ ê°œìˆ˜ (ì¡°ì ˆ ê°€ëŠ¥)
        news_list = []
        news_count_dict = {}  # ë‚ ì§œë³„ ì´ ë‰´ìŠ¤ ê°œìˆ˜ ì €ì¥
        
        import time
        
        try:
            news_iter = self.client.list_ticker_news(
                ticker=self.ticker,
                published_utc_gte=self.start_date,
                published_utc_lte=self.end_date,
                limit=100,
                sort='published_utc',
                order='asc'
            )
            
            total_collected = 0
            
            for item in news_iter:
                if total_collected >= self.news_limit:
                    break
                
                text = item.description if item.description else item.title
                if not text:
                    continue
                
                date = pd.to_datetime(item.published_utc).date()
                
                # 1. ë‚ ì§œë³„ ê°œìˆ˜ ì¹´ìš´íŒ… (ëª¨ë“  ë‰´ìŠ¤)
                news_count_dict[date] = news_count_dict.get(date, 0) + 1
                
                # 2. ë‚ ì§œë‹¹ ìµœëŒ€ ê°œìˆ˜ ì²´í¬
                current_date_count = len([n for n in news_list if n['Date'] == date])
                
                # 3. ë‚ ì§œë‹¹ max_per_dayê°œê¹Œì§€ë§Œ ì‹¤ì œ ì €ì¥ (ìƒ˜í”Œë§)
                if current_date_count < max_per_day:
                    news_list.append({
                        'Date': date,
                        'Text': text
                    })
                
                total_collected += 1
                
                # Rate Limit íšŒí”¼
                if total_collected % 5 == 0:
                    print(f"  -> Processed {total_collected} news, {len(news_list)} sampled...")
                    time.sleep(12)
                    
        except Exception as e:
            print(f"Error fetching news: {e}")
        
        # DataFrame ìƒì„±
        df_news = pd.DataFrame(news_list)
        
        # ë‰´ìŠ¤ ê°œìˆ˜ ì •ë³´ ì¶”ê°€
        df_count = pd.DataFrame([
            {'Date': date, 'News_Count': count} 
            for date, count in news_count_dict.items()
        ])
        
        print(f"\n  ğŸ“Š Total news found: {sum(news_count_dict.values())}")
        print(f"  ğŸ“ Sampled for analysis: {len(news_list)}")
        print(f"  ğŸ“… Unique dates: {len(news_count_dict)}")
        
        return df_news, df_count


    def calculate_sentiment(self, text_list):
        """
        FinBERT ê°ì„± ë¶„ì„ (-1 ~ 1 ìŠ¤ì¹¼ë¼)
        Core Logic 5.1: positive=prob, negative=-prob, neutral=0
        """
        print("\n[3/3] Calculating Sentiment Scores...")
        modified_scores = []
        
        batch_size = self.cfg.data.get('batch_size', 32)
        
        for i in tqdm(range(0, len(text_list), batch_size)):
            batch_texts = text_list[i:i+batch_size]
            
            inputs = self.tokenizer(
                batch_texts, 
                return_tensors="pt", 
                padding=True, 
                truncation=True, 
                max_length=512
            ).to(self.device)
            
            with torch.no_grad():
                outputs = self.model(**inputs)
                probabilities = F.softmax(outputs.logits, dim=1)
            
            for prob in probabilities:
                # ProsusAI/finbert: {0: 'positive', 1: 'negative', 2: 'neutral'}
                pos_prob = prob[0].item()
                neg_prob = prob[1].item()
                label_idx = torch.argmax(prob).item()
                
                # Modified_Score: -1 ~ 1 ìŠ¤ì¹¼ë¼
                if label_idx == 0:      # Positive
                    modified_scores.append(pos_prob)
                elif label_idx == 1:    # Negative
                    modified_scores.append(-neg_prob)
                else:                   # Neutral
                    modified_scores.append(0.0)
        
        return modified_scores

    def prepare_lstm_data(self):
        """
        LSTMìš© ë°ì´í„° ì¤€ë¹„ (Sentiment + News_Count í¬í•¨)
        """
        # 1. ì£¼ê°€ ë°ì´í„°
        df_price = self.fetch_prices()
        
        # 2. ë‰´ìŠ¤ ë°ì´í„° (ê°œìˆ˜ ì •ë³´ í¬í•¨)
        df_news, df_count = self.fetch_news_with_volume()
        
        if df_news.empty:
            print("âš ï¸ No news found. Using sentiment=0, count=0")
            df_price_reset = df_price.reset_index()
            df_price_reset['Sentiment'] = 0.0
            df_price_reset['News_Count'] = 0
        else:
            # 3. ê°ì„± ë¶„ì„ (ìƒ˜í”Œë§ëœ ë‰´ìŠ¤ë§Œ)
            sentiments = self.calculate_sentiment(df_news['Text'].tolist())
            df_news['Sentiment'] = sentiments
            
            # 4. ë‚ ì§œë³„ ê°ì„± í‰ê· 
            df_sentiment_avg = df_news.groupby('Date')['Sentiment'].mean().reset_index()
            df_sentiment_avg.columns = ['Date', 'Sentiment_Avg']
            
            # 5. ì£¼ê°€ + ê°ì„± + ë‰´ìŠ¤ê°œìˆ˜ ë³‘í•©
            df_price_reset = df_price.reset_index()
            df_merged = pd.merge(df_price_reset, df_sentiment_avg, on='Date', how='left')
            df_merged = pd.merge(df_merged, df_count, on='Date', how='left')
            
            # ë‰´ìŠ¤ ì—†ëŠ” ë‚  ì²˜ë¦¬
            df_merged['Sentiment_Avg'] = df_merged['Sentiment_Avg'].fillna(0.0)
            df_merged['News_Count'] = df_merged['News_Count'].fillna(0).astype(int)
            
            df_price_reset = df_merged
        
        # 6. Feature ì„ íƒ: OHLCV + Sentiment + News_Count (7ê°œ!)
        feature_cols = ['Open', 'High', 'Low', 'Close', 'Volume', 'Sentiment_Avg', 'News_Count']
        data = df_price_reset[feature_cols].values
        
        # 7. Scaling
        scaler = MinMaxScaler()
        data_scaled = scaler.fit_transform(data)
        
        # 8. Sliding Window
        X, y = [], []
        for i in range(len(data_scaled) - self.window_size):
            X.append(data_scaled[i : i + self.window_size])
            y.append(data_scaled[i + self.window_size, 3])  # Close Price
        
        X = np.array(X)  # (samples, window_size, 7)
        y = np.array(y)
        
        print(f"\nâœ… LSTM Data Ready: X.shape={X.shape}, y.shape={y.shape}")
        print(f"   Features: {feature_cols}")
        
        return X, y, scaler, df_price_reset
